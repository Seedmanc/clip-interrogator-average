{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jm8RYrLqvzz"
      },
      "source": [
        "# CLIP Interrogator for datasets by [@seedmanc](https://github.com/seedmanc)\n",
        "\n",
        "Want to figure out what prompt describes your dataset as a whole? Or pick a prompt opposite to it to test your lora's performance in out of distribution situations? The CLIP Interrogator is here to get you answers!\n",
        "\n",
        "<br>\n",
        "\n",
        "For Stable Diffusion 1.X choose the **ViT-L** model, for Stable Diffusion 2.0+ choose the **ViT-H** CLIP Model, SDXL needs either L or G versions, but L is faster. You can blank out the captioning model to only use CLIP, otherwise the top 2 most matching captions will be prepended to its output.\n",
        "\n",
        "This version is specialized for producing nice prompts for use with Stable Diffusion and achieves higher alignment between generated text prompt and source image.\n",
        "\n",
        "<br>\n",
        "\n",
        "If this notebook is helpful to you consider following me on [xitter](https://x.com/seedmanc) for more cool Ai stuff. ðŸ™‚\n",
        "\n",
        "And if you're looking for more AI art tools check out the [AI generative art tools list](https://pharmapsychotic.com/tools.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpPKQR40qvz2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "!git clone https://github.com/seedmanc/clip-interrogator-average.git\n",
        "!pip install open_clip_torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model_name = '' #@param [\"blip-base\", \"blip-large\", \"blip2-2.7b\", \"blip2-flan-t5-xl\", \"\"]\n",
        "clip_model_name = 'ViT-L-14/openai' #@param [\"ViT-L-14/openai\", \"ViT-H-14/laion2b_s32b_b79k\", \"ViT-g-14/laion2B-s34B-b88K\"]\n",
        "\n",
        "import sys\n",
        "from PIL import Image\n",
        "\n",
        "# Add the subdirectory to sys.path\n",
        "sys.path.append('clip-interrogator-average')\n",
        "from clip_interrogator import Config, Interrogator\n",
        "\n",
        "config = Config()\n",
        "config.clip_model_name = clip_model_name\n",
        "config.caption_model_name = caption_model_name\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "entries = ['artists','flavors']\n",
        "for entry in entries:\n",
        "  try:\n",
        "    hf_hub_download(\n",
        "        repo_id=\"seedmanc/clip-interrogator-cache\",\n",
        "        filename=f\"{clip_model_name.replace('/', '_').replace('@', '_')}_{entry}.safetensors\",\n",
        "        local_dir=\"clip-interrogator-average/cache\",\n",
        "    )\n",
        "  except:\n",
        "    print(f'No {entry} cache found for {clip_model_name}')\n",
        "\n",
        "ci = Interrogator(config)\n",
        "\n",
        "def image_to_prompt(files, mode):\n",
        "    images = [Image.open(i).convert('RGB') for i in files]\n",
        "    ci.config.chunk_size = 2048 if ci.config.clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "    ci.config.flavor_intermediate_count = 2048 if ci.config.clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "    prompt = ci.interrogate(images)\n",
        "    yield prompt, None, None\n",
        "    orth = ci.interrogate_orthogonal_fast(images)\n",
        "    yield prompt, orth, None\n",
        "    neg = ci.interrogate_negative(images)\n",
        "    yield prompt, orth, neg"
      ],
      "metadata": {
        "id": "uUExRfQEw7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf6qkFG6MPRj"
      },
      "outputs": [],
      "source": [
        "#@title Images to prompt! ðŸ–¼ï¸ðŸ–¼ï¸ðŸ–¼ï¸ -> ðŸ“\n",
        "import gradio as gr\n",
        "\n",
        "def toggleRun(files):\n",
        "  Ready = files is not None and len(files) > 0\n",
        "  return gr.Button(variant='primary' if Ready else 'secondary', interactive = Ready), '', '', ''\n",
        "\n",
        "def prompt_tab():\n",
        "    with gr.Column():\n",
        "        with gr.Row():\n",
        "            images = gr.Files(label=\"Image\",file_types=[\"image\",\".webp\"])\n",
        "            with gr.Column():\n",
        "                button = gr.Button(\"Generate prompts\", interactive=False)\n",
        "                prompt = gr.Textbox(label=\"Prompt\",lines=4)\n",
        "                orthprompt = gr.Textbox(label=\"Neutral\",lines=3)\n",
        "                negprompt = gr.Textbox(label=\"Negative\",lines=2)\n",
        "    button.click(image_to_prompt, inputs=[images], outputs=[prompt,orthprompt,negprompt])\n",
        "    images.change(toggleRun, images, [button,prompt,orthprompt,negprompt])\n",
        "\n",
        "with gr.Blocks() as ui:\n",
        "    prompt_tab()\n",
        "\n",
        "ui.launch( debug=True,show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1f51d5616d3bc2b87a82685314c5be1ec9a49b6e0cb1f707bfa2acb6c45f3e5f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}