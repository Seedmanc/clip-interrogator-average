{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jm8RYrLqvzz"
      },
      "source": [
        "# CLIP Interrogator for datasets by [@seedmanc](https://github.com/seedmanc)\n",
        "\n",
        "Want to figure out what prompt describes your dataset as a whole? Or pick a prompt opposite to it to test your lora's performance in out of distribution situations? The CLIP Interrogator is here to get you answers!\n",
        "\n",
        "<br>\n",
        "\n",
        "For Stable Diffusion 1.X choose the **ViT-L** model, for Stable Diffusion 2.0+ choose the **ViT-H** CLIP Model, SDXL needs either L or G versions, but L is faster. You can blank out the captioning model to only use CLIP, otherwise the top 2 most matching captions will be prepended to its output.\n",
        "\n",
        "This version is specialized for producing nice prompts for use with Stable Diffusion and achieves higher alignment between generated text prompt and source image.\n",
        "\n",
        "<br>\n",
        "\n",
        "If this notebook is helpful to you consider following me on [xitter](https://x.com/seedmanc) for more cool Ai stuff. üôÇ\n",
        "\n",
        "And if you're looking for more AI art tools check out the [AI generative art tools list](https://pharmapsychotic.com/tools.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpPKQR40qvz2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "!git clone https://github.com/seedmanc/clip-interrogator-average.git\n",
        "!pip install open_clip_torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "caption_model_name = 'blip2-2.7b' #@param [\"blip-base\", \"blip-large\", \"blip2-2.7b\", \"blip2-flan-t5-xl\", \"\"]\n",
        "clip_model_name = 'ViT-L-14/openai' #@param [\"ViT-L-14/openai\", \"ViT-H-14/laion2b_s32b_b79k\", \"ViT-g-14/laion2B-s34B-b88K\"]\n",
        "\n",
        "import sys\n",
        "from PIL import Image\n",
        "\n",
        "# Add the subdirectory to sys.path\n",
        "sys.path.append('clip-interrogator-average')\n",
        "from clip_interrogator import Config, Interrogator\n",
        "\n",
        "config = Config()\n",
        "config.clip_model_name = clip_model_name\n",
        "config.caption_model_name = caption_model_name\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "entries = ['artists','flavors']\n",
        "for entry in entries:\n",
        "  try:\n",
        "    hf_hub_download(\n",
        "        repo_id=\"seedmanc/clip-interrogator-cache\",\n",
        "        filename=f\"{clip_model_name.replace('/', '_').replace('@', '_')}_{entry}.safetensors\",\n",
        "        local_dir=\"clip-interrogator-average/cache\",\n",
        "    )\n",
        "  except:\n",
        "    print(f'No {entry} cache found for {clip_model_name}')\n",
        "\n",
        "ci = Interrogator(config)\n",
        "\n",
        "def _tb(txt,scr):\n",
        "  info = f\"Score: {scr:.3f}\" if scr is not None else None\n",
        "  return gr.Textbox(value=txt, info=info)\n",
        "\n",
        "def image_to_prompt(files, mode):\n",
        "  if files is None or len(files) == 0:\n",
        "    yield _tb('',None), _tb('',None), _tb('',None), None\n",
        "    return\n",
        "\n",
        "  images = [Image.open(i).convert('RGB') for i in files]\n",
        "  ci.config.chunk_size = 2048 if ci.config.clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "  ci.config.flavor_intermediate_count = 2048 if ci.config.clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "  res = ci.interrogate(images)\n",
        "  prompt, score, imge = None, None, None\n",
        "  for value in res:\n",
        "    prompt, score, imge = value\n",
        "    yield _tb(prompt, score), None, None, imge\n",
        "\n",
        "  orth, score2 = ci.interrogate_orthogonal_fast(images)\n",
        "  yield _tb(prompt, score), _tb(orth, score2), None, imge\n",
        "  neg, score3 = ci.interrogate_negative(images)\n",
        "  yield _tb(prompt, score), _tb(orth, score2), _tb(neg, score3), imge"
      ],
      "metadata": {
        "id": "uUExRfQEw7Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf6qkFG6MPRj"
      },
      "outputs": [],
      "source": [
        "#@title Images to prompt! üñºÔ∏èüñºÔ∏èüñºÔ∏è -> üìù\n",
        "import gradio as gr\n",
        "\n",
        "def prompt_tab():\n",
        "    with gr.Column():\n",
        "        with gr.Row():\n",
        "            images = gr.Files(label=\"Image\",file_types=[\"image\",\".webp\"])\n",
        "            with gr.Column():\n",
        "                prompt = gr.Textbox(label=\"Prompt\",lines=3, show_copy_button=True)\n",
        "                orthprompt = gr.Textbox(label=\"Neutral\",lines=3)\n",
        "                negprompt = gr.Textbox(label=\"Negative\",lines=2)\n",
        "                img = gr.Image(label=\"Most representative image\")\n",
        "    images.change(image_to_prompt, images, [prompt,orthprompt,negprompt,img])\n",
        "\n",
        "with gr.Blocks(fill_height=True) as ui:\n",
        "    prompt_tab()\n",
        "\n",
        "ui.launch( debug=True,show_error=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1f51d5616d3bc2b87a82685314c5be1ec9a49b6e0cb1f707bfa2acb6c45f3e5f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}